{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ea02c18",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/autoharness/CarTool-Instruct/blob/main/fine_tuning/Fine_Tuning_Car_Tool_Instruct_with_Hugging_Face.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd5cb25",
   "metadata": {},
   "source": [
    "# Fine-tune for CarTool-Instruct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe24c84c",
   "metadata": {},
   "source": [
    "This notebook provides a workflow for fine-tuning large language models on the CarTool-Instruct dataset to enhance function-calling capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53858dc",
   "metadata": {},
   "source": [
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77315d37",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "%pip install torch\n",
    "%pip install -U transformers==4.57.1 trl==0.25.1 datasets==4.4.1 peft==0.7.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eecb65d3",
   "metadata": {},
   "source": [
    "You need to create a new Colab secret in the left toolbar. Specify `HF_TOKEN` as the 'Name', add your unique token as the 'Value', and toggle 'Notebook access' on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5758d6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from google.colab import userdata\n",
    "from huggingface_hub import login\n",
    "\n",
    "# Login into Hugging Face Hub\n",
    "hf_token = userdata.get('HF_TOKEN')\n",
    "login(hf_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14feb00b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "import gc\n",
    "import os\n",
    "import torch\n",
    "from abc import ABC, abstractmethod\n",
    "from huggingface_hub import hf_hub_download\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers.pipelines.pt_utils import KeyDataset\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "from trl import SFTConfig, SFTTrainer\n",
    "from peft import LoraConfig, PeftModel\n",
    "\n",
    "USE_LORA = False # Full Fine-tuning or Low-Rank Adaptation\n",
    "MODEL_NAME = \"google/functiongemma-270m-it\"\n",
    "MAX_TRAINING_SAMPLES = 9509\n",
    "OUTPUT_DIR = f\"/content/{MODEL_NAME.replace(\"/\", \"_\")}_{\"lora\" if USE_LORA else \"fft\"}_{MAX_TRAINING_SAMPLES}\"\n",
    "DATA_FILE = hf_hub_download(repo_id=\"autoharness/CarTool-Instruct\", filename=\"dataset.jsonl\", repo_type=\"dataset\")\n",
    "CHAT_TEMPLATE_PATH = None # \"/content/chat_template_for_qwen.jinja\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d69e9185",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b1276b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    device_map=\"auto\",\n",
    "    dtype=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "if CHAT_TEMPLATE_PATH and os.path.exists(CHAT_TEMPLATE_PATH):\n",
    "    print(f\"Loading custom chat template from: {CHAT_TEMPLATE_PATH}\")\n",
    "    with open(CHAT_TEMPLATE_PATH, 'r', encoding='utf-8') as f:\n",
    "        custom_template = f.read()\n",
    "        tokenizer.chat_template = custom_template.strip()\n",
    "else:\n",
    "    print(f\"Using default model chat template.\")\n",
    "\n",
    "print(f\"Device: {base_model.device}\")\n",
    "print(f\"DType:  {base_model.dtype}\")\n",
    "print(f\"Attention implementation: {base_model.config._attn_implementation}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "044b735f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def apply_format(sample, tokenizer):\n",
    "    template_inputs = json.loads(sample[\"text\"])\n",
    "\n",
    "    prompt_and_completion = tokenizer.apply_chat_template(\n",
    "        template_inputs[\"messages\"],\n",
    "        tools=template_inputs[\"tools\"],\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=False,\n",
    "    )\n",
    "\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        template_inputs[\"messages\"][:-1],\n",
    "        tools=template_inputs[\"tools\"],\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "    )\n",
    "\n",
    "    completion = prompt_and_completion[len(prompt) :]\n",
    "\n",
    "    return {\n",
    "        \"prompt\": prompt,\n",
    "        \"completion\": completion,\n",
    "        \"split\": template_inputs[\"metadata\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e4bb14",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"text\", data_files=DATA_FILE, encoding=\"utf-8\")[\n",
    "    \"train\"\n",
    "].shuffle()\n",
    "processed_dataset = dataset.map(lambda x: apply_format(x, tokenizer))\n",
    "\n",
    "train_dataset = processed_dataset.filter(lambda example: example[\"split\"] == \"train\")\n",
    "train_dataset = train_dataset.select(range(min(len(train_dataset), MAX_TRAINING_SAMPLES)))\n",
    "eval_dataset = processed_dataset.filter(lambda example: example[\"split\"] == \"eval\")\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset)}, Eval samples: {len(eval_dataset)}\")\n",
    "\n",
    "combined_subset = concatenate_datasets([train_dataset, eval_dataset])\n",
    "longest_example = max(\n",
    "    combined_subset,\n",
    "    key=lambda example: len(example[\"prompt\"] + example[\"completion\"]),\n",
    ")\n",
    "longest_example_token_count = len(\n",
    "    tokenizer.tokenize(longest_example[\"prompt\"] + longest_example[\"completion\"])\n",
    ")\n",
    "max_token_count = longest_example_token_count + 100\n",
    "\n",
    "print(f\"The longest example length is {len(longest_example['prompt'] + longest_example['completion'])} with {longest_example_token_count} tokens.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d13d33",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "if USE_LORA:\n",
    "    print(\">>> CONFIGURING FOR LoRA TRAINING\")\n",
    "    learning_rate = 2e-4\n",
    "    peft_config = LoraConfig(\n",
    "        r=8,\n",
    "        lora_alpha=16,\n",
    "        lora_dropout=0.05,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "        target_modules=[\n",
    "            \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "            \"gate_proj\", \"up_proj\", \"down_proj\"\n",
    "        ],\n",
    "    )\n",
    "else:\n",
    "    print(\">>> CONFIGURING FOR FULL FINE-TUNING\")\n",
    "    learning_rate = 1e-5\n",
    "    peft_config = None\n",
    "\n",
    "args = SFTConfig(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    num_train_epochs=2,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=32,\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=50,\n",
    "    eval_strategy=\"no\",\n",
    "    save_strategy=\"no\",\n",
    "    learning_rate=learning_rate,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    max_length=max_token_count,\n",
    "    gradient_checkpointing=True,\n",
    "    packing=False,\n",
    "    optim=\"adamw_torch_fused\",\n",
    "    bf16=True,\n",
    "    completion_only_loss=True,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "base_model.config.pad_token_id = tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2ab432",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=base_model,\n",
    "    args=args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    peft_config=peft_config,\n",
    "    processing_class=tokenizer,\n",
    ")\n",
    "trainer.train()\n",
    "\n",
    "trainer.save_model(OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "\n",
    "print(f\"Training finished. Model/Adapter saved to {OUTPUT_DIR}\")\n",
    "\n",
    "if USE_LORA:\n",
    "    print(\"\\n>>> STARTING LORA MERGE PROCESS...\")\n",
    "\n",
    "    # Clean up memory before reloading the model\n",
    "    del trainer\n",
    "    del base_model\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    print(f\">>> Reloading base model: {MODEL_NAME}\")\n",
    "    base_model_reload = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=\"auto\"\n",
    "    )\n",
    "\n",
    "    # Load the adapter\n",
    "    model_to_merge = PeftModel.from_pretrained(base_model_reload, OUTPUT_DIR)\n",
    "\n",
    "    # Merge and Unload\n",
    "    print(\">>> Merging...\")\n",
    "    merged_model = model_to_merge.merge_and_unload()\n",
    "\n",
    "    # Save Merged Model\n",
    "    merged_output_dir = os.path.join(OUTPUT_DIR, \"merged\")\n",
    "    print(f\">>> Saving merged model to: {merged_output_dir}\")\n",
    "    merged_model.save_pretrained(merged_output_dir)\n",
    "    tokenizer.save_pretrained(merged_output_dir)\n",
    "\n",
    "    print(\">>> Merge Complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcfe5df0",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "from pathlib import Path\n",
    "\n",
    "zip_filename = f\"{Path(OUTPUT_DIR).name}.zip\"\n",
    "\n",
    "!zip -r -q {zip_filename} {OUTPUT_DIR}\n",
    "\n",
    "files.download(zip_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7752e2e6",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4889181",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def infer_value_type(value_str):\n",
    "    \"\"\"\n",
    "    Parses a raw string value into its appropriate Python type.\n",
    "    \"\"\"\n",
    "    value_str = value_str.strip()\n",
    "\n",
    "    if value_str.startswith(\"<escape>\") and value_str.endswith(\"<escape>\"):\n",
    "        return value_str[8:-8]  # Remove tags and return as string\n",
    "\n",
    "    if value_str == \"true\":\n",
    "        return True\n",
    "    if value_str == \"false\":\n",
    "        return False\n",
    "    if value_str == \"null\":\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        return int(value_str)\n",
    "    except ValueError:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        return float(value_str)\n",
    "    except ValueError:\n",
    "        pass\n",
    "\n",
    "    return value_str\n",
    "\n",
    "def extract_function_call_json(model_output):\n",
    "    \"\"\"\n",
    "    For models producing JSON blocks (e.g., Gemma).\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    # Try to extract JSON from code blocks\n",
    "    json_pattern = r\"```tool_code\\s*(.*?)\\s*```\"\n",
    "    match = re.search(json_pattern, model_output, re.DOTALL)\n",
    "\n",
    "    json_str = \"\"\n",
    "    if match:\n",
    "        json_str = match.group(1).strip()\n",
    "    else:\n",
    "        # Fallback: look for outer brackets\n",
    "        start_array = model_output.find(\"[\")\n",
    "        start_obj = model_output.find(\"{\")\n",
    "\n",
    "        if start_array != -1 and (start_obj == -1 or start_array < start_obj):\n",
    "            end_idx = model_output.rfind(\"]\")\n",
    "            if end_idx != -1:\n",
    "                json_str = model_output[start_array : end_idx + 1]\n",
    "        elif start_obj != -1:\n",
    "            end_idx = model_output.rfind(\"}\")\n",
    "            if end_idx != -1:\n",
    "                json_str = model_output[start_obj : end_idx + 1]\n",
    "\n",
    "    if not json_str:\n",
    "        return results\n",
    "\n",
    "    try:\n",
    "        data = json.loads(json_str)\n",
    "        if isinstance(data, dict):\n",
    "            data = [data]\n",
    "\n",
    "        if isinstance(data, list):\n",
    "            for item in data:\n",
    "                func_name = item.get(\"name\")\n",
    "                params = item.get(\"arguments\", {})\n",
    "                if func_name:\n",
    "                    results.append(\n",
    "                        {\"function\": {\"name\": func_name, \"arguments\": params}}\n",
    "                    )\n",
    "    except json.JSONDecodeError:\n",
    "        pass\n",
    "\n",
    "    return results\n",
    "\n",
    "def extract_function_call_functiongemma(model_output):\n",
    "    \"\"\"\n",
    "    Parses a string containing specific function call markers and returns\n",
    "    a list of function call objects. Here is an example of the obejct:\n",
    "\n",
    "    ```\n",
    "    call:open_map{query:San Francisco}\n",
    "    ```\n",
    "\n",
    "    Args:\n",
    "        model_output (str): The model output string.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of dictionaries representing the function calls.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    # Pattern to extract the full content of a single function call\n",
    "    # Flags: DOTALL allows matching across newlines if necessary\n",
    "    call_pattern = r\"<start_function_call>(.*?)<end_function_call>\"\n",
    "    raw_calls = re.findall(call_pattern, model_output, re.DOTALL)\n",
    "    for raw_call in raw_calls:\n",
    "        # Check if the content starts with 'call:'\n",
    "        if not raw_call.strip().startswith(\"call:\"):\n",
    "            continue\n",
    "\n",
    "        # Extract function name\n",
    "        # Expected format: call:func_name{...}\n",
    "        try:\n",
    "            # Split only on the first brace to separate name and args\n",
    "            pre_brace, args_segment = raw_call.split(\"{\", 1)\n",
    "\n",
    "            function_name = pre_brace.replace(\"call:\", \"\").strip()\n",
    "\n",
    "            # Remove the trailing closing brace '}'\n",
    "            args_content = args_segment.strip()\n",
    "            if args_content.endswith(\"}\"):\n",
    "                args_content = args_content[:-1]\n",
    "\n",
    "            arguments = {}\n",
    "\n",
    "            arg_pattern = r\"(?:^|,)\\s*(?P<key>[^:]+):(?P<value><escape>.*?<escape>|[^,]*)\"\n",
    "            arg_matches = re.finditer(arg_pattern, args_content, re.DOTALL)\n",
    "            for match in arg_matches:\n",
    "                key = match.group(\"key\").strip()\n",
    "                raw_value = match.group(\"value\").strip()\n",
    "\n",
    "                arguments[key] = infer_value_type(raw_value)\n",
    "\n",
    "            results.append(\n",
    "                {\"function\": {\"name\": function_name, \"arguments\": arguments}}\n",
    "            )\n",
    "\n",
    "        except ValueError:\n",
    "            # Handles cases where syntax might be malformed (e.g., missing '{')\n",
    "            continue\n",
    "\n",
    "    return results\n",
    "\n",
    "def get_eval_logs(dataset, pipe, extract_fn):\n",
    "    batch_size = 32\n",
    "    logs = []\n",
    "\n",
    "    print(f\"Starting prediction on {len(dataset)} samples...\")\n",
    "\n",
    "    # Iterate over the pipeline results\n",
    "    for i, output in enumerate(pipe(KeyDataset(dataset, \"prompt\"), batch_size=batch_size)):\n",
    "        orig_data = dataset[i][\"text\"]\n",
    "        messages = json.loads(orig_data)[\"messages\"]\n",
    "        user_message = messages[1]\n",
    "        assistant_first_message = messages[2]\n",
    "        input_prompt = dataset[i][\"prompt\"]\n",
    "\n",
    "        # Extract generated text.\n",
    "        model_output_only = output[0][\"generated_text\"][len(input_prompt):].strip()\n",
    "\n",
    "        output_fc = extract_fn(model_output_only)\n",
    "\n",
    "        logs.append(\n",
    "            {\n",
    "                \"user\": user_message[\"content\"],\n",
    "                \"target_fc\": assistant_first_message.get(\"tool_calls\", []),\n",
    "                \"target_text\": assistant_first_message.get(\"content\"),\n",
    "                \"output_fc\": output_fc,\n",
    "                \"output_text\": model_output_only,\n",
    "            }\n",
    "        )\n",
    "\n",
    "        if (i + 1) % batch_size == 0:\n",
    "            print(f\"Eval process: {(i + 1) * 100.0 / len(dataset):.2f}%\")\n",
    "\n",
    "    return logs\n",
    "\n",
    "def get_scored_data_frame(dataset, pipe, extract_fn):\n",
    "    logs = get_eval_logs(dataset, pipe, extract_fn)\n",
    "    logs_df = pd.DataFrame.from_records(logs)\n",
    "\n",
    "    scored = pd.DataFrame()\n",
    "    scored[\"user\"] = logs_df[\"user\"]\n",
    "    scored[\"target_names\"] = logs_df[\"target_fc\"].apply(\n",
    "        lambda x: [fc[\"function\"][\"name\"] for fc in x]\n",
    "    )\n",
    "    scored[\"output_names\"] = logs_df[\"output_fc\"].apply(\n",
    "        lambda x: [fc[\"function\"][\"name\"] for fc in x]\n",
    "    )\n",
    "\n",
    "    # Sort arguments for consistent comparison\n",
    "    scored[\"target_arguments\"] = logs_df[\"target_fc\"].apply(\n",
    "        lambda x: [dict(sorted(fc[\"function\"][\"arguments\"].items())) for fc in x]\n",
    "    )\n",
    "    scored[\"output_arguments\"] = logs_df[\"output_fc\"].apply(\n",
    "        lambda x: [dict(sorted(fc[\"function\"][\"arguments\"].items())) for fc in x]\n",
    "    )\n",
    "\n",
    "    scored[\"target_text\"] = logs_df[\"target_text\"]\n",
    "    scored[\"output_text\"] = logs_df[\"output_text\"]\n",
    "\n",
    "    scored[\"correct_names\"] = scored[\"target_names\"] == scored[\"output_names\"]\n",
    "    scored[\"correct_arguments\"] = (\n",
    "        scored[\"target_arguments\"] == scored[\"output_arguments\"]\n",
    "    )\n",
    "    scored[\"correct\"] = scored[\"correct_names\"] & scored[\"correct_arguments\"]\n",
    "\n",
    "    return scored\n",
    "\n",
    "def review(scored):\n",
    "    scored[\"incorrect_names\"] = scored[\"target_names\"] != scored[\"output_names\"]\n",
    "    scored[\"incorrect_arguments\"] = (\n",
    "        scored[\"target_arguments\"] != scored[\"output_arguments\"]\n",
    "    )\n",
    "    scored[\"incorrect\"] = scored[\"incorrect_names\"] | scored[\"incorrect_arguments\"]\n",
    "\n",
    "    print(f\"\\nTotal Incorrect: {len(scored[scored['incorrect']])}\")\n",
    "    for index, row in scored[scored[\"incorrect\"]].iterrows():\n",
    "        print(f\"Sample #{index} Prompt: {row['user']}\")\n",
    "        print(f\"Expected: {row['target_names']}, {row['target_arguments']}\")\n",
    "        print(f\"Actual  : {row['output_names']}, {row['output_arguments']}\")\n",
    "        print(\"---------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f89ab61",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "if 'trainer' in globals(): del trainer\n",
    "if 'base_model' in globals(): del base_model\n",
    "if 'merged_model' in globals(): del merged_model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "if USE_LORA:\n",
    "    eval_model_path = os.path.join(OUTPUT_DIR, \"merged\")\n",
    "    print(f\"Loading MERGED LoRA model from: {eval_model_path}\")\n",
    "else:\n",
    "    eval_model_path = OUTPUT_DIR\n",
    "    print(f\"Loading Full Fine-Tuned model from: {eval_model_path}\")\n",
    "\n",
    "target_model = AutoModelForCausalLM.from_pretrained(\n",
    "    eval_model_path,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(eval_model_path)\n",
    "\n",
    "if \"functiongemma\" in MODEL_NAME:\n",
    "    current_extractor = extract_function_call_functiongemma\n",
    "else:\n",
    "    current_extractor = extract_function_call_json\n",
    "\n",
    "print(f\"Running evaluation using extractor: {current_extractor.__name__}\")\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=target_model,\n",
    "    tokenizer=tokenizer,\n",
    "    temperature=0.001\n",
    ")\n",
    "\n",
    "print(f\"Starting evaluation on {len(eval_dataset)} validation samples...\")\n",
    "model_scored = get_scored_data_frame(\n",
    "    eval_dataset,\n",
    "    pipe,\n",
    "    extract_fn=current_extractor\n",
    ")\n",
    "\n",
    "corrected_values = model_scored[\"correct\"]\n",
    "accuracy = corrected_values.mean()\n",
    "\n",
    "print(f\"\\n{'='*30}\")\n",
    "print(f\"EVALUATION RESULTS\")\n",
    "print(f\"{'='*30}\")\n",
    "print(f\"Model Path: {eval_model_path}\")\n",
    "print(f\"Final Accuracy: {accuracy:.2%}\")\n",
    "print(f\"{'='*30}\")\n",
    "\n",
    "review(model_scored)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
